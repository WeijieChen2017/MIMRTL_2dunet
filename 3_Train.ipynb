{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pytorch-CycleGAN-and-pix2pix/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-450078e766ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch-CycleGAN-and-pix2pix/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pytorch-CycleGAN-and-pix2pix/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = ['6','8','8R']\n",
    "basic_command_1 = \"python train.py --dataroot ./datasets/2d_enhanced_sk\"\n",
    "basic_command_2 = \" --name 2d_enhanced_sk\"\n",
    "basic_command_3 = \" --model pix2pix --direction BtoA --batch_size 80 --gpu_ids 0 --save_epoch_freq 25 --n_epochs 100 --n_epochs_decay 100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py --dataroot ./datasets/2d_enhanced_sk6 --name 2d_enhanced_sk6 --model pix2pix --direction BtoA --batch_size 80 --gpu_ids 0 --save_epoch_freq 25 --n_epochs 100 --n_epochs_decay 100\n",
      "python train.py --dataroot ./datasets/2d_enhanced_sk8 --name 2d_enhanced_sk8 --model pix2pix --direction BtoA --batch_size 80 --gpu_ids 0 --save_epoch_freq 25 --n_epochs 100 --n_epochs_decay 100\n",
      "python train.py --dataroot ./datasets/2d_enhanced_sk8R --name 2d_enhanced_sk8R --model pix2pix --direction BtoA --batch_size 80 --gpu_ids 0 --save_epoch_freq 25 --n_epochs 100 --n_epochs_decay 100\n"
     ]
    }
   ],
   "source": [
    "for sk in para:\n",
    "    command = basic_command_1 + sk + basic_command_2 + sk + basic_command_3\n",
    "    print(command)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --dataroot ./datasets/subj01_t1b_inv --name subj01_inv_pix2pix --model pix2pix --direction BtoA --batch_size 72 --gpu_ids 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 20                            \t[default: 1]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/2d_enhanced_sk8R_2x\t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: BtoA                          \t[default: AtoB]\n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: vanilla                       \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: cycle_gan]\n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: 2d_enhanced_sk8R_2x_512       \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_512                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 25                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 568\n",
      "initialize network with normal\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           3,072\n",
      "         LeakyReLU-2         [-1, 64, 512, 512]               0\n",
      "            Conv2d-3        [-1, 128, 256, 256]         131,072\n",
      "       BatchNorm2d-4        [-1, 128, 256, 256]             256\n",
      "         LeakyReLU-5        [-1, 128, 256, 256]               0\n",
      "            Conv2d-6        [-1, 256, 128, 128]         524,288\n",
      "       BatchNorm2d-7        [-1, 256, 128, 128]             512\n",
      "         LeakyReLU-8        [-1, 256, 128, 128]               0\n",
      "            Conv2d-9          [-1, 512, 64, 64]       2,097,152\n",
      "      BatchNorm2d-10          [-1, 512, 64, 64]           1,024\n",
      "        LeakyReLU-11          [-1, 512, 64, 64]               0\n",
      "           Conv2d-12          [-1, 512, 32, 32]       4,194,304\n",
      "      BatchNorm2d-13          [-1, 512, 32, 32]           1,024\n",
      "        LeakyReLU-14          [-1, 512, 32, 32]               0\n",
      "           Conv2d-15          [-1, 512, 16, 16]       4,194,304\n",
      "      BatchNorm2d-16          [-1, 512, 16, 16]           1,024\n",
      "        LeakyReLU-17          [-1, 512, 16, 16]               0\n",
      "           Conv2d-18            [-1, 512, 8, 8]       4,194,304\n",
      "      BatchNorm2d-19            [-1, 512, 8, 8]           1,024\n",
      "        LeakyReLU-20            [-1, 512, 8, 8]               0\n",
      "           Conv2d-21            [-1, 512, 4, 4]       4,194,304\n",
      "      BatchNorm2d-22            [-1, 512, 4, 4]           1,024\n",
      "        LeakyReLU-23            [-1, 512, 4, 4]               0\n",
      "           Conv2d-24            [-1, 512, 2, 2]       4,194,304\n",
      "      BatchNorm2d-25            [-1, 512, 2, 2]           1,024\n",
      "        LeakyReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 1, 1]       4,194,304\n",
      "             ReLU-28            [-1, 512, 1, 1]               0\n",
      "  ConvTranspose2d-29            [-1, 512, 2, 2]       4,194,304\n",
      "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
      "UnetSkipConnectionBlock-31           [-1, 1024, 2, 2]               0\n",
      "             ReLU-32           [-1, 1024, 2, 2]               0\n",
      "  ConvTranspose2d-33            [-1, 512, 4, 4]       8,388,608\n",
      "      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n",
      "          Dropout-35            [-1, 512, 4, 4]               0\n",
      "UnetSkipConnectionBlock-36           [-1, 1024, 4, 4]               0\n",
      "             ReLU-37           [-1, 1024, 4, 4]               0\n",
      "  ConvTranspose2d-38            [-1, 512, 8, 8]       8,388,608\n",
      "      BatchNorm2d-39            [-1, 512, 8, 8]           1,024\n",
      "          Dropout-40            [-1, 512, 8, 8]               0\n",
      "UnetSkipConnectionBlock-41           [-1, 1024, 8, 8]               0\n",
      "             ReLU-42           [-1, 1024, 8, 8]               0\n",
      "  ConvTranspose2d-43          [-1, 512, 16, 16]       8,388,608\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "          Dropout-45          [-1, 512, 16, 16]               0\n",
      "UnetSkipConnectionBlock-46         [-1, 1024, 16, 16]               0\n",
      "             ReLU-47         [-1, 1024, 16, 16]               0\n",
      "  ConvTranspose2d-48          [-1, 512, 32, 32]       8,388,608\n",
      "      BatchNorm2d-49          [-1, 512, 32, 32]           1,024\n",
      "          Dropout-50          [-1, 512, 32, 32]               0\n",
      "UnetSkipConnectionBlock-51         [-1, 1024, 32, 32]               0\n",
      "             ReLU-52         [-1, 1024, 32, 32]               0\n",
      "  ConvTranspose2d-53          [-1, 512, 64, 64]       8,388,608\n",
      "      BatchNorm2d-54          [-1, 512, 64, 64]           1,024\n",
      "          Dropout-55          [-1, 512, 64, 64]               0\n",
      "UnetSkipConnectionBlock-56         [-1, 1024, 64, 64]               0\n",
      "             ReLU-57         [-1, 1024, 64, 64]               0\n",
      "  ConvTranspose2d-58        [-1, 256, 128, 128]       4,194,304\n",
      "      BatchNorm2d-59        [-1, 256, 128, 128]             512\n",
      "UnetSkipConnectionBlock-60        [-1, 512, 128, 128]               0\n",
      "             ReLU-61        [-1, 512, 128, 128]               0\n",
      "  ConvTranspose2d-62        [-1, 128, 256, 256]       1,048,576\n",
      "      BatchNorm2d-63        [-1, 128, 256, 256]             256\n",
      "UnetSkipConnectionBlock-64        [-1, 256, 256, 256]               0\n",
      "             ReLU-65        [-1, 256, 256, 256]               0\n",
      "  ConvTranspose2d-66         [-1, 64, 512, 512]         262,144\n",
      "      BatchNorm2d-67         [-1, 64, 512, 512]             128\n",
      "UnetSkipConnectionBlock-68        [-1, 128, 512, 512]               0\n",
      "             ReLU-69        [-1, 128, 512, 512]               0\n",
      "  ConvTranspose2d-70        [-1, 3, 1024, 1024]           6,147\n",
      "             Tanh-71        [-1, 3, 1024, 1024]               0\n",
      "UnetSkipConnectionBlock-72        [-1, 3, 1024, 1024]               0\n",
      "    UnetGenerator-73        [-1, 3, 1024, 1024]               0\n",
      "     DataParallel-74        [-1, 3, 1024, 1024]               0\n",
      "================================================================\n",
      "Total params: 79,583,875\n",
      "Trainable params: 79,583,875\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 12.00\n",
      "Forward/backward pass size (MB): 2221.27\n",
      "Params size (MB): 303.59\n",
      "Estimated Total Size (MB): 2536.86\n",
      "----------------------------------------------------------------\n",
      "\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 79.584 M\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "create web directory ./checkpoints/2d_enhanced_sk8R_2x_512/web...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot ./datasets/2d_enhanced_sk8R_2x --name 2d_enhanced_sk8R_2x_512 --model pix2pix --direction BtoA --batch_size 20 --gpu_ids 0 --save_epoch_freq 25 --n_epochs 100 --n_epochs_decay 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 20                            \t[default: 1]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 1024                          \t[default: 256]\n",
      "                 dataroot: ./datasets/2d_enhanced_sk8R_2x\t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: BtoA                          \t[default: AtoB]\n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: vanilla                       \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 1024                          \t[default: 286]\n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: cycle_gan]\n",
      "                 n_epochs: 50                            \t[default: 100]\n",
      "           n_epochs_decay: 50                            \t[default: 100]\n",
      "               n_layers_D: 3                             \n",
      "                     name: 2d_enhanced_sk8R_2x_512       \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_512                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 25                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 568\n",
      "initialize network with normal\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           3,072\n",
      "         LeakyReLU-2         [-1, 64, 512, 512]               0\n",
      "            Conv2d-3        [-1, 128, 256, 256]         131,072\n",
      "       BatchNorm2d-4        [-1, 128, 256, 256]             256\n",
      "         LeakyReLU-5        [-1, 128, 256, 256]               0\n",
      "            Conv2d-6        [-1, 256, 128, 128]         524,288\n",
      "       BatchNorm2d-7        [-1, 256, 128, 128]             512\n",
      "         LeakyReLU-8        [-1, 256, 128, 128]               0\n",
      "            Conv2d-9          [-1, 512, 64, 64]       2,097,152\n",
      "      BatchNorm2d-10          [-1, 512, 64, 64]           1,024\n",
      "        LeakyReLU-11          [-1, 512, 64, 64]               0\n",
      "           Conv2d-12          [-1, 512, 32, 32]       4,194,304\n",
      "      BatchNorm2d-13          [-1, 512, 32, 32]           1,024\n",
      "        LeakyReLU-14          [-1, 512, 32, 32]               0\n",
      "           Conv2d-15          [-1, 512, 16, 16]       4,194,304\n",
      "      BatchNorm2d-16          [-1, 512, 16, 16]           1,024\n",
      "        LeakyReLU-17          [-1, 512, 16, 16]               0\n",
      "           Conv2d-18            [-1, 512, 8, 8]       4,194,304\n",
      "      BatchNorm2d-19            [-1, 512, 8, 8]           1,024\n",
      "        LeakyReLU-20            [-1, 512, 8, 8]               0\n",
      "           Conv2d-21            [-1, 512, 4, 4]       4,194,304\n",
      "      BatchNorm2d-22            [-1, 512, 4, 4]           1,024\n",
      "        LeakyReLU-23            [-1, 512, 4, 4]               0\n",
      "           Conv2d-24            [-1, 512, 2, 2]       4,194,304\n",
      "      BatchNorm2d-25            [-1, 512, 2, 2]           1,024\n",
      "        LeakyReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 1, 1]       4,194,304\n",
      "             ReLU-28            [-1, 512, 1, 1]               0\n",
      "  ConvTranspose2d-29            [-1, 512, 2, 2]       4,194,304\n",
      "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
      "UnetSkipConnectionBlock-31           [-1, 1024, 2, 2]               0\n",
      "             ReLU-32           [-1, 1024, 2, 2]               0\n",
      "  ConvTranspose2d-33            [-1, 512, 4, 4]       8,388,608\n",
      "      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n",
      "          Dropout-35            [-1, 512, 4, 4]               0\n",
      "UnetSkipConnectionBlock-36           [-1, 1024, 4, 4]               0\n",
      "             ReLU-37           [-1, 1024, 4, 4]               0\n",
      "  ConvTranspose2d-38            [-1, 512, 8, 8]       8,388,608\n",
      "      BatchNorm2d-39            [-1, 512, 8, 8]           1,024\n",
      "          Dropout-40            [-1, 512, 8, 8]               0\n",
      "UnetSkipConnectionBlock-41           [-1, 1024, 8, 8]               0\n",
      "             ReLU-42           [-1, 1024, 8, 8]               0\n",
      "  ConvTranspose2d-43          [-1, 512, 16, 16]       8,388,608\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "          Dropout-45          [-1, 512, 16, 16]               0\n",
      "UnetSkipConnectionBlock-46         [-1, 1024, 16, 16]               0\n",
      "             ReLU-47         [-1, 1024, 16, 16]               0\n",
      "  ConvTranspose2d-48          [-1, 512, 32, 32]       8,388,608\n",
      "      BatchNorm2d-49          [-1, 512, 32, 32]           1,024\n",
      "          Dropout-50          [-1, 512, 32, 32]               0\n",
      "UnetSkipConnectionBlock-51         [-1, 1024, 32, 32]               0\n",
      "             ReLU-52         [-1, 1024, 32, 32]               0\n",
      "  ConvTranspose2d-53          [-1, 512, 64, 64]       8,388,608\n",
      "      BatchNorm2d-54          [-1, 512, 64, 64]           1,024\n",
      "          Dropout-55          [-1, 512, 64, 64]               0\n",
      "UnetSkipConnectionBlock-56         [-1, 1024, 64, 64]               0\n",
      "             ReLU-57         [-1, 1024, 64, 64]               0\n",
      "  ConvTranspose2d-58        [-1, 256, 128, 128]       4,194,304\n",
      "      BatchNorm2d-59        [-1, 256, 128, 128]             512\n",
      "UnetSkipConnectionBlock-60        [-1, 512, 128, 128]               0\n",
      "             ReLU-61        [-1, 512, 128, 128]               0\n",
      "  ConvTranspose2d-62        [-1, 128, 256, 256]       1,048,576\n",
      "      BatchNorm2d-63        [-1, 128, 256, 256]             256\n",
      "UnetSkipConnectionBlock-64        [-1, 256, 256, 256]               0\n",
      "             ReLU-65        [-1, 256, 256, 256]               0\n",
      "  ConvTranspose2d-66         [-1, 64, 512, 512]         262,144\n",
      "      BatchNorm2d-67         [-1, 64, 512, 512]             128\n",
      "UnetSkipConnectionBlock-68        [-1, 128, 512, 512]               0\n",
      "             ReLU-69        [-1, 128, 512, 512]               0\n",
      "  ConvTranspose2d-70        [-1, 3, 1024, 1024]           6,147\n",
      "             Tanh-71        [-1, 3, 1024, 1024]               0\n",
      "UnetSkipConnectionBlock-72        [-1, 3, 1024, 1024]               0\n",
      "    UnetGenerator-73        [-1, 3, 1024, 1024]               0\n",
      "     DataParallel-74        [-1, 3, 1024, 1024]               0\n",
      "================================================================\n",
      "Total params: 79,583,875\n",
      "Trainable params: 79,583,875\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 12.00\n",
      "Forward/backward pass size (MB): 2221.27\n",
      "Params size (MB): 303.59\n",
      "Estimated Total Size (MB): 2536.86\n",
      "----------------------------------------------------------------\n",
      "\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 79.584 M\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "create web directory ./checkpoints/2d_enhanced_sk8R_2x_512/web...\n",
      "End of epoch 1 / 100 \t Time Taken: 218 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 2 / 100 \t Time Taken: 182 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 3 / 100 \t Time Taken: 166 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 4 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 5 / 100 \t Time Taken: 164 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 6 / 100 \t Time Taken: 151 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 7 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 8 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 9, total_iters 5000)\n",
      "End of epoch 9 / 100 \t Time Taken: 164 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 10 / 100 \t Time Taken: 157 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 11 / 100 \t Time Taken: 175 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 12 / 100 \t Time Taken: 167 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 13 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 14 / 100 \t Time Taken: 151 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 15 / 100 \t Time Taken: 157 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 16 / 100 \t Time Taken: 161 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 17 / 100 \t Time Taken: 160 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 18, total_iters 10000)\n",
      "End of epoch 18 / 100 \t Time Taken: 174 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 19 / 100 \t Time Taken: 152 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 20 / 100 \t Time Taken: 159 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 21 / 100 \t Time Taken: 153 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 22 / 100 \t Time Taken: 151 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 23 / 100 \t Time Taken: 152 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 24 / 100 \t Time Taken: 154 sec\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 25, iters 14500\n",
      "End of epoch 25 / 100 \t Time Taken: 167 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 26, total_iters 15000)\n",
      "End of epoch 26 / 100 \t Time Taken: 171 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 27 / 100 \t Time Taken: 159 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 28 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 29 / 100 \t Time Taken: 154 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 30 / 100 \t Time Taken: 160 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 31 / 100 \t Time Taken: 157 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 32 / 100 \t Time Taken: 151 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 33 / 100 \t Time Taken: 155 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 34 / 100 \t Time Taken: 154 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 35, total_iters 20000)\n",
      "End of epoch 35 / 100 \t Time Taken: 158 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 36 / 100 \t Time Taken: 159 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 37 / 100 \t Time Taken: 156 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 38 / 100 \t Time Taken: 157 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 39 / 100 \t Time Taken: 160 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 40 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 41 / 100 \t Time Taken: 159 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 42 / 100 \t Time Taken: 158 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 43 / 100 \t Time Taken: 170 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 44, total_iters 25000)\n",
      "End of epoch 44 / 100 \t Time Taken: 169 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 45 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 46 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 47 / 100 \t Time Taken: 161 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 48 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 49 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 50, iters 29000\n",
      "End of epoch 50 / 100 \t Time Taken: 179 sec\n",
      "learning rate = 0.0001961\n",
      "End of epoch 51 / 100 \t Time Taken: 164 sec\n",
      "learning rate = 0.0001922\n",
      "saving the latest model (epoch 52, total_iters 30000)\n",
      "End of epoch 52 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0001882\n",
      "End of epoch 53 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0001843\n",
      "End of epoch 54 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0001804\n",
      "End of epoch 55 / 100 \t Time Taken: 164 sec\n",
      "learning rate = 0.0001765\n",
      "End of epoch 56 / 100 \t Time Taken: 160 sec\n",
      "learning rate = 0.0001725\n",
      "End of epoch 57 / 100 \t Time Taken: 158 sec\n",
      "learning rate = 0.0001686\n",
      "End of epoch 58 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0001647\n",
      "End of epoch 59 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0001608\n",
      "End of epoch 60 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0001569\n",
      "saving the latest model (epoch 61, total_iters 35000)\n",
      "End of epoch 61 / 100 \t Time Taken: 166 sec\n",
      "learning rate = 0.0001529\n",
      "End of epoch 62 / 100 \t Time Taken: 157 sec\n",
      "learning rate = 0.0001490\n",
      "End of epoch 63 / 100 \t Time Taken: 156 sec\n",
      "learning rate = 0.0001451\n",
      "End of epoch 64 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0001412\n",
      "End of epoch 65 / 100 \t Time Taken: 154 sec\n",
      "learning rate = 0.0001373\n",
      "End of epoch 66 / 100 \t Time Taken: 167 sec\n",
      "learning rate = 0.0001333\n",
      "End of epoch 67 / 100 \t Time Taken: 160 sec\n",
      "learning rate = 0.0001294\n",
      "End of epoch 68 / 100 \t Time Taken: 156 sec\n",
      "learning rate = 0.0001255\n",
      "saving the latest model (epoch 69, total_iters 40000)\n",
      "End of epoch 69 / 100 \t Time Taken: 169 sec\n",
      "learning rate = 0.0001216\n",
      "End of epoch 70 / 100 \t Time Taken: 168 sec\n",
      "learning rate = 0.0001176\n",
      "End of epoch 71 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0001137\n",
      "End of epoch 72 / 100 \t Time Taken: 155 sec\n",
      "learning rate = 0.0001098\n",
      "End of epoch 73 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0001059\n",
      "End of epoch 74 / 100 \t Time Taken: 164 sec\n",
      "learning rate = 0.0001020\n",
      "saving the model at the end of epoch 75, iters 43500\n",
      "End of epoch 75 / 100 \t Time Taken: 173 sec\n",
      "learning rate = 0.0000980\n",
      "End of epoch 76 / 100 \t Time Taken: 170 sec\n",
      "learning rate = 0.0000941\n",
      "End of epoch 77 / 100 \t Time Taken: 176 sec\n",
      "learning rate = 0.0000902\n",
      "saving the latest model (epoch 78, total_iters 45000)\n",
      "End of epoch 78 / 100 \t Time Taken: 166 sec\n",
      "learning rate = 0.0000863\n",
      "End of epoch 79 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0000824\n",
      "End of epoch 80 / 100 \t Time Taken: 168 sec\n",
      "learning rate = 0.0000784\n",
      "End of epoch 81 / 100 \t Time Taken: 158 sec\n",
      "learning rate = 0.0000745\n",
      "End of epoch 82 / 100 \t Time Taken: 161 sec\n",
      "learning rate = 0.0000706\n",
      "End of epoch 83 / 100 \t Time Taken: 168 sec\n",
      "learning rate = 0.0000667\n",
      "End of epoch 84 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0000627\n",
      "End of epoch 85 / 100 \t Time Taken: 162 sec\n",
      "learning rate = 0.0000588\n",
      "End of epoch 86 / 100 \t Time Taken: 158 sec\n",
      "learning rate = 0.0000549\n",
      "saving the latest model (epoch 87, total_iters 50000)\n",
      "End of epoch 87 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0000510\n",
      "End of epoch 88 / 100 \t Time Taken: 158 sec\n",
      "learning rate = 0.0000471\n",
      "End of epoch 89 / 100 \t Time Taken: 155 sec\n",
      "learning rate = 0.0000431\n",
      "End of epoch 90 / 100 \t Time Taken: 151 sec\n",
      "learning rate = 0.0000392\n",
      "End of epoch 91 / 100 \t Time Taken: 160 sec\n",
      "learning rate = 0.0000353\n",
      "End of epoch 92 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0000314\n",
      "End of epoch 93 / 100 \t Time Taken: 163 sec\n",
      "learning rate = 0.0000275\n",
      "End of epoch 94 / 100 \t Time Taken: 164 sec\n",
      "learning rate = 0.0000235\n",
      "saving the latest model (epoch 95, total_iters 55000)\n",
      "End of epoch 95 / 100 \t Time Taken: 169 sec\n",
      "learning rate = 0.0000196\n",
      "End of epoch 96 / 100 \t Time Taken: 164 sec\n",
      "learning rate = 0.0000157\n",
      "End of epoch 97 / 100 \t Time Taken: 161 sec\n",
      "learning rate = 0.0000118\n",
      "End of epoch 98 / 100 \t Time Taken: 165 sec\n",
      "learning rate = 0.0000078\n",
      "End of epoch 99 / 100 \t Time Taken: 159 sec\n",
      "learning rate = 0.0000039\n",
      "saving the model at the end of epoch 100, iters 58000\n",
      "End of epoch 100 / 100 \t Time Taken: 167 sec\n",
      "learning rate = 0.0000000\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot ./datasets/2d_enhanced_sk8R_2x --name 2d_enhanced_sk8R_2x_512 --model pix2pix --direction BtoA --batch_size 20 --gpu_ids 0 --save_epoch_freq 25 --n_epochs 50 --n_epochs_decay 50 --crop_size 1024 --load_size 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 16                            \t[default: 1]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/2d_enhanced_sk8R   \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 300                           \t[default: 100]\n",
      "           n_epochs_decay: 300                           \t[default: 100]\n",
      "               n_layers_D: 3                             \n",
      "                     name: sk8R                          \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \t[default: resnet_9blocks]\n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 100                           \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 568\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 54.410 M\n",
      "[Network G_B] Total number of parameters : 54.410 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "Setting up a new session...\n",
      "create web directory ./checkpoints/sk8R/web...\n",
      "End of epoch 1 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 2 / 600 \t Time Taken: 123 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 3 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 4 / 600 \t Time Taken: 123 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 5 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 6 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 7 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 8 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 9 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 10 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 11 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 12 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 13 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 14 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 15 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 16 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 17 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 18, total_iters 10000)\n",
      "End of epoch 18 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 19 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 20 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 21 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 22 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 23 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 24 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 25 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 26 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 27 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 28 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 29 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 30 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 31 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 32 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 33 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 34 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 35, total_iters 20000)\n",
      "End of epoch 35 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 36 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 37 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 38 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 39 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 40 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 41 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 42 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 43 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 44 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 45 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 46 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 47 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 48 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 49 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 50 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 51 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 52 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 53, total_iters 30000)\n",
      "End of epoch 53 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 54 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 55 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 56 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 57 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 58 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 59 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 60 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 61 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 62 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 63 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 64 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 65 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 66 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 67 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 68 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 69 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 70, total_iters 40000)\n",
      "End of epoch 70 / 600 \t Time Taken: 132 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 71 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 72 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 73 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 74 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 75 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 76 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 77 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 78 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 79 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 80 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 81 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 82 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 83 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 84 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 85 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 86 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 87, total_iters 50000)\n",
      "End of epoch 87 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 88 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 89 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 90 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 91 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 92 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 93 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 94 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 95 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 96 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 97 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 98 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 99 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 100, iters 57600\n",
      "End of epoch 100 / 600 \t Time Taken: 139 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 101 / 600 \t Time Taken: 124 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 102 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 103 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 104 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 105, total_iters 60000)\n",
      "End of epoch 105 / 600 \t Time Taken: 132 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 106 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 107 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 108 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 109 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 110 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 111 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 112 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 113 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 114 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 115 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 116 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 117 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 118 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 119 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 120 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 121 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 122, total_iters 70000)\n",
      "End of epoch 122 / 600 \t Time Taken: 132 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 123 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 124 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 125 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 126 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 127 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 128 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 129 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 130 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 131 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 132 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 133 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 134 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 135 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 136 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 137 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 138 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 139, total_iters 80000)\n",
      "End of epoch 139 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 140 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 141 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 142 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 143 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 144 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 145 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 146 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 147 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 148 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 149 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 150 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 151 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 152 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 153 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 154 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 155 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 156 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 157, total_iters 90000)\n",
      "End of epoch 157 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 158 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 159 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 160 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 161 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 162 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 163 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 164 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 165 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 166 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 167 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 168 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 169 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 170 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 171 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 172 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 173 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 174, total_iters 100000)\n",
      "End of epoch 174 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 175 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 176 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 177 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 178 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 179 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 180 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 181 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 182 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 183 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 184 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 185 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 186 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 187 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 188 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 189 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 190 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 191, total_iters 110000)\n",
      "End of epoch 191 / 600 \t Time Taken: 136 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 192 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 193 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 194 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 195 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 196 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 197 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 198 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 199 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 200, iters 115200\n",
      "End of epoch 200 / 600 \t Time Taken: 141 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 201 / 600 \t Time Taken: 125 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 202 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 203 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 204 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 205 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 206 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 207 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 208 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 209, total_iters 120000)\n",
      "End of epoch 209 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 210 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 211 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 212 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 213 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 214 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 215 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 216 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 217 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 218 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 219 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 220 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 221 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 222 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 223 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 224 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 225 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 226, total_iters 130000)\n",
      "End of epoch 226 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 227 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 228 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 229 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 230 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 231 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 232 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 233 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 234 / 600 \t Time Taken: 126 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 235 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 236 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 237 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 238 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 239 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 240 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 241 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 242 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 243 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 244, total_iters 140000)\n",
      "End of epoch 244 / 600 \t Time Taken: 134 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 245 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 246 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 247 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 248 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 249 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 250 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 251 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 252 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 253 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 254 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 255 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 256 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 257 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 258 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 259 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 260 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 261, total_iters 150000)\n",
      "End of epoch 261 / 600 \t Time Taken: 134 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 262 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 263 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 264 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 265 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 266 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 267 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 268 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 269 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 270 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 271 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 272 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 273 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 274 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 275 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 276 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 277 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 278, total_iters 160000)\n",
      "End of epoch 278 / 600 \t Time Taken: 138 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 279 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 280 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 281 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 282 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 283 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 284 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 285 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 286 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 287 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 288 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 289 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 290 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 291 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 292 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 293 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 294 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 295 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "saving the latest model (epoch 296, total_iters 170000)\n",
      "End of epoch 296 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 297 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 298 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 299 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 300, iters 172800\n",
      "End of epoch 300 / 600 \t Time Taken: 142 sec\n",
      "learning rate = 0.0001993\n",
      "End of epoch 301 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0001987\n",
      "End of epoch 302 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001980\n",
      "End of epoch 303 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001973\n",
      "End of epoch 304 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0001967\n",
      "End of epoch 305 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001960\n",
      "End of epoch 306 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0001953\n",
      "End of epoch 307 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001947\n",
      "End of epoch 308 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0001940\n",
      "End of epoch 309 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001934\n",
      "End of epoch 310 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001927\n",
      "End of epoch 311 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001920\n",
      "End of epoch 312 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001914\n",
      "saving the latest model (epoch 313, total_iters 180000)\n",
      "End of epoch 313 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0001907\n",
      "End of epoch 314 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001900\n",
      "End of epoch 315 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001894\n",
      "End of epoch 316 / 600 \t Time Taken: 132 sec\n",
      "learning rate = 0.0001887\n",
      "End of epoch 317 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0001880\n",
      "End of epoch 318 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001874\n",
      "End of epoch 319 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001867\n",
      "End of epoch 320 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001860\n",
      "End of epoch 321 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001854\n",
      "End of epoch 322 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001847\n",
      "End of epoch 323 / 600 \t Time Taken: 132 sec\n",
      "learning rate = 0.0001841\n",
      "End of epoch 324 / 600 \t Time Taken: 127 sec\n",
      "learning rate = 0.0001834\n",
      "End of epoch 325 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001827\n",
      "End of epoch 326 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001821\n",
      "End of epoch 327 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001814\n",
      "End of epoch 328 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001807\n",
      "End of epoch 329 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001801\n",
      "saving the latest model (epoch 330, total_iters 190000)\n",
      "End of epoch 330 / 600 \t Time Taken: 139 sec\n",
      "learning rate = 0.0001794\n",
      "End of epoch 331 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001787\n",
      "End of epoch 332 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001781\n",
      "End of epoch 333 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001774\n",
      "End of epoch 334 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001767\n",
      "End of epoch 335 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001761\n",
      "End of epoch 336 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001754\n",
      "End of epoch 337 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0001748\n",
      "End of epoch 338 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001741\n",
      "End of epoch 339 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001734\n",
      "End of epoch 340 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001728\n",
      "End of epoch 341 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001721\n",
      "End of epoch 342 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001714\n",
      "End of epoch 343 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001708\n",
      "End of epoch 344 / 600 \t Time Taken: 133 sec\n",
      "learning rate = 0.0001701\n",
      "End of epoch 345 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001694\n",
      "End of epoch 346 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001688\n",
      "End of epoch 347 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001681\n",
      "saving the latest model (epoch 348, total_iters 200000)\n",
      "End of epoch 348 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0001674\n",
      "End of epoch 349 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001668\n",
      "End of epoch 350 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001661\n",
      "End of epoch 351 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001654\n",
      "End of epoch 352 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001648\n",
      "End of epoch 353 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001641\n",
      "End of epoch 354 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001635\n",
      "End of epoch 355 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001628\n",
      "End of epoch 356 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001621\n",
      "End of epoch 357 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001615\n",
      "End of epoch 358 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001608\n",
      "End of epoch 359 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001601\n",
      "End of epoch 360 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001595\n",
      "End of epoch 361 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001588\n",
      "End of epoch 362 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001581\n",
      "End of epoch 363 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001575\n",
      "End of epoch 364 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001568\n",
      "saving the latest model (epoch 365, total_iters 210000)\n",
      "End of epoch 365 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0001561\n",
      "End of epoch 366 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001555\n",
      "End of epoch 367 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001548\n",
      "End of epoch 368 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001542\n",
      "End of epoch 369 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001535\n",
      "End of epoch 370 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001528\n",
      "End of epoch 371 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001522\n",
      "End of epoch 372 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001515\n",
      "End of epoch 373 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001508\n",
      "End of epoch 374 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001502\n",
      "End of epoch 375 / 600 \t Time Taken: 134 sec\n",
      "learning rate = 0.0001495\n",
      "End of epoch 376 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001488\n",
      "End of epoch 377 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001482\n",
      "End of epoch 378 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001475\n",
      "End of epoch 379 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001468\n",
      "End of epoch 380 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001462\n",
      "End of epoch 381 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001455\n",
      "saving the latest model (epoch 382, total_iters 220000)\n",
      "End of epoch 382 / 600 \t Time Taken: 141 sec\n",
      "learning rate = 0.0001449\n",
      "End of epoch 383 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001442\n",
      "End of epoch 384 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001435\n",
      "End of epoch 385 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001429\n",
      "End of epoch 386 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 387 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001415\n",
      "End of epoch 388 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001409\n",
      "End of epoch 389 / 600 \t Time Taken: 134 sec\n",
      "learning rate = 0.0001402\n",
      "End of epoch 390 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001395\n",
      "End of epoch 391 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001389\n",
      "End of epoch 392 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001382\n",
      "End of epoch 393 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001375\n",
      "End of epoch 394 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001369\n",
      "End of epoch 395 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001362\n",
      "End of epoch 396 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0001355\n",
      "End of epoch 397 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001349\n",
      "End of epoch 398 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001342\n",
      "End of epoch 399 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001336\n",
      "saving the latest model (epoch 400, total_iters 230000)\n",
      "saving the model at the end of epoch 400, iters 230400\n",
      "End of epoch 400 / 600 \t Time Taken: 151 sec\n",
      "learning rate = 0.0001329\n",
      "End of epoch 401 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001322\n",
      "End of epoch 402 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001316\n",
      "End of epoch 403 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0001309\n",
      "End of epoch 404 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001302\n",
      "End of epoch 405 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001296\n",
      "End of epoch 406 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001289\n",
      "End of epoch 407 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001282\n",
      "End of epoch 408 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001276\n",
      "End of epoch 409 / 600 \t Time Taken: 128 sec\n",
      "learning rate = 0.0001269\n",
      "End of epoch 410 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0001262\n",
      "End of epoch 411 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001256\n",
      "End of epoch 412 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001249\n",
      "End of epoch 413 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001243\n",
      "End of epoch 414 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0001236\n",
      "End of epoch 415 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001229\n",
      "End of epoch 416 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001223\n",
      "saving the latest model (epoch 417, total_iters 240000)\n",
      "End of epoch 417 / 600 \t Time Taken: 136 sec\n",
      "learning rate = 0.0001216\n",
      "End of epoch 418 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001209\n",
      "End of epoch 419 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0001203\n",
      "End of epoch 420 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001196\n",
      "End of epoch 421 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001189\n",
      "End of epoch 422 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001183\n",
      "End of epoch 423 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001176\n",
      "End of epoch 424 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001169\n",
      "End of epoch 425 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001163\n",
      "End of epoch 426 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001156\n",
      "End of epoch 427 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001150\n",
      "End of epoch 428 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001143\n",
      "End of epoch 429 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001136\n",
      "End of epoch 430 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001130\n",
      "End of epoch 431 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001123\n",
      "End of epoch 432 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001116\n",
      "End of epoch 433 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001110\n",
      "End of epoch 434 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001103\n",
      "saving the latest model (epoch 435, total_iters 250000)\n",
      "End of epoch 435 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0001096\n",
      "End of epoch 436 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001090\n",
      "End of epoch 437 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0001083\n",
      "End of epoch 438 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001076\n",
      "End of epoch 439 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0001070\n",
      "End of epoch 440 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001063\n",
      "End of epoch 441 / 600 \t Time Taken: 136 sec\n",
      "learning rate = 0.0001056\n",
      "End of epoch 442 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001050\n",
      "End of epoch 443 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001043\n",
      "End of epoch 444 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001037\n",
      "End of epoch 445 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001030\n",
      "End of epoch 446 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001023\n",
      "End of epoch 447 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001017\n",
      "End of epoch 448 / 600 \t Time Taken: 136 sec\n",
      "learning rate = 0.0001010\n",
      "End of epoch 449 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0001003\n",
      "End of epoch 450 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000997\n",
      "End of epoch 451 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000990\n",
      "saving the latest model (epoch 452, total_iters 260000)\n",
      "End of epoch 452 / 600 \t Time Taken: 136 sec\n",
      "learning rate = 0.0000983\n",
      "End of epoch 453 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000977\n",
      "End of epoch 454 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000970\n",
      "End of epoch 455 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0000963\n",
      "End of epoch 456 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000957\n",
      "End of epoch 457 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000950\n",
      "End of epoch 458 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000944\n",
      "End of epoch 459 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000937\n",
      "End of epoch 460 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000930\n",
      "End of epoch 461 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000924\n",
      "End of epoch 462 / 600 \t Time Taken: 135 sec\n",
      "learning rate = 0.0000917\n",
      "End of epoch 463 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000910\n",
      "End of epoch 464 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000904\n",
      "End of epoch 465 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000897\n",
      "End of epoch 466 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000890\n",
      "End of epoch 467 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000884\n",
      "End of epoch 468 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000877\n",
      "saving the latest model (epoch 469, total_iters 270000)\n",
      "End of epoch 469 / 600 \t Time Taken: 143 sec\n",
      "learning rate = 0.0000870\n",
      "End of epoch 470 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000864\n",
      "End of epoch 471 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000857\n",
      "End of epoch 472 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000850\n",
      "End of epoch 473 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000844\n",
      "End of epoch 474 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000837\n",
      "End of epoch 475 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000831\n",
      "End of epoch 476 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000824\n",
      "End of epoch 477 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000817\n",
      "End of epoch 478 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000811\n",
      "End of epoch 479 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000804\n",
      "End of epoch 480 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000797\n",
      "End of epoch 481 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000791\n",
      "End of epoch 482 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000784\n",
      "End of epoch 483 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000777\n",
      "End of epoch 484 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000771\n",
      "End of epoch 485 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000764\n",
      "End of epoch 486 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000757\n",
      "saving the latest model (epoch 487, total_iters 280000)\n",
      "End of epoch 487 / 600 \t Time Taken: 138 sec\n",
      "learning rate = 0.0000751\n",
      "End of epoch 488 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000744\n",
      "End of epoch 489 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000738\n",
      "End of epoch 490 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000731\n",
      "End of epoch 491 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000724\n",
      "End of epoch 492 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000718\n",
      "End of epoch 493 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000711\n",
      "End of epoch 494 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000704\n",
      "End of epoch 495 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000698\n",
      "End of epoch 496 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 497 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000684\n",
      "End of epoch 498 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000678\n",
      "End of epoch 499 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000671\n",
      "saving the model at the end of epoch 500, iters 288000\n",
      "End of epoch 500 / 600 \t Time Taken: 150 sec\n",
      "learning rate = 0.0000664\n",
      "End of epoch 501 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000658\n",
      "End of epoch 502 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000651\n",
      "End of epoch 503 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000645\n",
      "saving the latest model (epoch 504, total_iters 290000)\n",
      "End of epoch 504 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0000638\n",
      "End of epoch 505 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000631\n",
      "End of epoch 506 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000625\n",
      "End of epoch 507 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0000618\n",
      "End of epoch 508 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000611\n",
      "End of epoch 509 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000605\n",
      "End of epoch 510 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000598\n",
      "End of epoch 511 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000591\n",
      "End of epoch 512 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000585\n",
      "End of epoch 513 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000578\n",
      "End of epoch 514 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0000571\n",
      "End of epoch 515 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000565\n",
      "End of epoch 516 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000558\n",
      "End of epoch 517 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000551\n",
      "End of epoch 518 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000545\n",
      "End of epoch 519 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000538\n",
      "End of epoch 520 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000532\n",
      "saving the latest model (epoch 521, total_iters 300000)\n",
      "End of epoch 521 / 600 \t Time Taken: 144 sec\n",
      "learning rate = 0.0000525\n",
      "End of epoch 522 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000518\n",
      "End of epoch 523 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000512\n",
      "End of epoch 524 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000505\n",
      "End of epoch 525 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000498\n",
      "End of epoch 526 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000492\n",
      "End of epoch 527 / 600 \t Time Taken: 129 sec\n",
      "learning rate = 0.0000485\n",
      "End of epoch 528 / 600 \t Time Taken: 136 sec\n",
      "learning rate = 0.0000478\n",
      "End of epoch 529 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000472\n",
      "End of epoch 530 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000465\n",
      "End of epoch 531 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000458\n",
      "End of epoch 532 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000452\n",
      "End of epoch 533 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000445\n",
      "End of epoch 534 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000439\n",
      "End of epoch 535 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0000432\n",
      "End of epoch 536 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000425\n",
      "End of epoch 537 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000419\n",
      "End of epoch 538 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000412\n",
      "saving the latest model (epoch 539, total_iters 310000)\n",
      "End of epoch 539 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0000405\n",
      "End of epoch 540 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000399\n",
      "End of epoch 541 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000392\n",
      "End of epoch 542 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000385\n",
      "End of epoch 543 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000379\n",
      "End of epoch 544 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000372\n",
      "End of epoch 545 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000365\n",
      "End of epoch 546 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000359\n",
      "End of epoch 547 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000352\n",
      "End of epoch 548 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000346\n",
      "End of epoch 549 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000339\n",
      "End of epoch 550 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000332\n",
      "End of epoch 551 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000326\n",
      "End of epoch 552 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000319\n",
      "End of epoch 553 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000312\n",
      "End of epoch 554 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000306\n",
      "End of epoch 555 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000299\n",
      "saving the latest model (epoch 556, total_iters 320000)\n",
      "End of epoch 556 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0000292\n",
      "End of epoch 557 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000286\n",
      "End of epoch 558 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000279\n",
      "End of epoch 559 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000272\n",
      "End of epoch 560 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000266\n",
      "End of epoch 561 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000259\n",
      "End of epoch 562 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000252\n",
      "End of epoch 563 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000246\n",
      "End of epoch 564 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000239\n",
      "End of epoch 565 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000233\n",
      "End of epoch 566 / 600 \t Time Taken: 138 sec\n",
      "learning rate = 0.0000226\n",
      "End of epoch 567 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000219\n",
      "End of epoch 568 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000213\n",
      "End of epoch 569 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000206\n",
      "End of epoch 570 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000199\n",
      "End of epoch 571 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000193\n",
      "End of epoch 572 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000186\n",
      "saving the latest model (epoch 573, total_iters 330000)\n",
      "End of epoch 573 / 600 \t Time Taken: 145 sec\n",
      "learning rate = 0.0000179\n",
      "End of epoch 574 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000173\n",
      "End of epoch 575 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000166\n",
      "End of epoch 576 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000159\n",
      "End of epoch 577 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000153\n",
      "End of epoch 578 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000146\n",
      "End of epoch 579 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000140\n",
      "End of epoch 580 / 600 \t Time Taken: 138 sec\n",
      "learning rate = 0.0000133\n",
      "End of epoch 581 / 600 \t Time Taken: 130 sec\n",
      "learning rate = 0.0000126\n",
      "End of epoch 582 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000120\n",
      "End of epoch 583 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000113\n",
      "End of epoch 584 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000106\n",
      "End of epoch 585 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000100\n",
      "End of epoch 586 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000093\n",
      "End of epoch 587 / 600 \t Time Taken: 139 sec\n",
      "learning rate = 0.0000086\n",
      "End of epoch 588 / 600 \t Time Taken: 137 sec\n",
      "learning rate = 0.0000080\n",
      "End of epoch 589 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000073\n",
      "End of epoch 590 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000066\n",
      "saving the latest model (epoch 591, total_iters 340000)\n",
      "End of epoch 591 / 600 \t Time Taken: 138 sec\n",
      "learning rate = 0.0000060\n",
      "End of epoch 592 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000053\n",
      "End of epoch 593 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000047\n",
      "End of epoch 594 / 600 \t Time Taken: 139 sec\n",
      "learning rate = 0.0000040\n",
      "End of epoch 595 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000033\n",
      "End of epoch 596 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000027\n",
      "End of epoch 597 / 600 \t Time Taken: 131 sec\n",
      "learning rate = 0.0000020\n",
      "End of epoch 598 / 600 \t Time Taken: 132 sec\n",
      "learning rate = 0.0000013\n",
      "End of epoch 599 / 600 \t Time Taken: 132 sec\n",
      "learning rate = 0.0000007\n",
      "saving the model at the end of epoch 600, iters 345600\n",
      "End of epoch 600 / 600 \t Time Taken: 153 sec\n",
      "learning rate = 0.0000000\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot ./datasets/2d_enhanced_sk8R --name sk8R --model cycle_gan --netG unet_256 --batch_size 16 --save_epoch_freq 100 --n_epochs 300 --n_epochs_decay 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
